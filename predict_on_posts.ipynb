{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d306c113-cddb-4222-8123-51aa57a6b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and variables\n",
    "import datetime\n",
    "import praw\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from credentials import p_id, p_agent, p_secret, p_pass\n",
    "\n",
    "total_post_limit = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d1e679b-a95e-4ee2-b0dd-80c8cb89c57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Reddit user: Adopolis23\n"
     ]
    }
   ],
   "source": [
    "#connect to reddit API\n",
    "#check authentication\n",
    "#select subreddit\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=p_id,\n",
    "    client_secret=p_secret,\n",
    "    password=p_pass,\n",
    "    user_agent='scrapper_bot',\n",
    "    username=p_agent,\n",
    ")\n",
    "\n",
    "print(\"Logged in as Reddit user: {}\".format(reddit.user.me()))\n",
    "\n",
    "#define date helper funciton\n",
    "#function to return date of post\n",
    "def get_date(submission):\n",
    "    time = submission.created\n",
    "    return datetime.datetime.fromtimestamp(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72fe2f88-c8da-4832-84f7-072f1b876bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime.datetime(2024, 6, 1, 0, 0, 0) \n",
    "\n",
    "tickers = [\"TSLA\"]\n",
    "stock_price = yf.download(tickers,  start = start_date , end = datetime.datetime.now())\n",
    "stock_price = stock_price['Close']\n",
    "\n",
    "stock_data_dates = list(stock_price.index.date)\n",
    "#stock_data_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a4a3b4dd-01bf-40d6-a9b1-fa77d723ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "sub_name = 'Stocks'\n",
    "terms_of_interest = ['TSLA', 'Tesla', 'TESLA']\n",
    "buffer_size = 20\n",
    "\n",
    "# Subreddit to search\n",
    "subreddit = reddit.subreddit(sub_name) \n",
    "\n",
    "#for each post in the subreddit\n",
    "for submission in subreddit.new(limit=total_post_limit):\n",
    "    submission_date = get_date(submission).date()\n",
    "    \n",
    "    #make sure it is on one of the days we are checking\n",
    "    if submission_date not in stock_data_dates:\n",
    "        continue\n",
    "    if submission_date < stock_data_dates[0]:\n",
    "        break\n",
    "    if submission_date not in data:\n",
    "        data[submission_date] = []\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    total_text = submission.title + ' ' + submission.selftext\n",
    "    total_text_list = total_text.split()\n",
    "    \n",
    "    for i, word in enumerate(total_text_list):\n",
    "        word = word.replace('.', '').replace('[', '').replace(',', '').replace('?', '').replace(']', '')\n",
    "        \n",
    "        if word in terms_of_interest:\n",
    "            \n",
    "            left_side = i - int(buffer_size/2)\n",
    "            right_side = i + int(buffer_size/2)\n",
    "\n",
    "            if left_side < 0:\n",
    "                left_side = 0\n",
    "\n",
    "            if right_side > len(total_text_list)-1:\n",
    "                right_side = len(total_text_list)-1\n",
    "\n",
    "            context_string = total_text_list[left_side:right_side]\n",
    "            data[submission_date].append(context_string)\n",
    "            \n",
    "            \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "77839d3a-37eb-439d-8174-e90428a82ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42 # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7416ee37-f5df-4a31-98dc-f30aa44baac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "\u001b[1m84125825/84125825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "\n",
    "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
    "                                  untar=True, cache_dir='.',\n",
    "                                  cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "# remove unused folders to make it easier to load the data\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4c49a879-af82-46e8-a75b-0467d0311737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 20000 reviews for train\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# 5000 reviews for validation\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# 25000 reviews for test\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test',\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2d6090b1-e599-48d3-ae8e-56eaab6371b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
       "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
       "      dtype='<U14')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_ds.map(lambda text, label: text))\n",
    "\n",
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8c3e3f6d-96c5-4adc-aebd-4a0f8a10acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "522d91fa-44b9-479b-aabb-8a2d63261661",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "824a8276-bdaf-4eb9-acdd-c2d83efd5a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 212ms/step - accuracy: 0.5357 - loss: 0.6691 - val_accuracy: 0.7521 - val_loss: 0.4628\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=1,\n",
    "                    validation_data=val_ds,\n",
    "                    validation_steps=1,\n",
    "                    callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4adeb6c5-964c-4b71-b0cc-88687a94125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('rnn/Models/model1.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a62b7760-1f4b-496c-9c2c-2ebe3714850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end. With just one week left in the second quarter, Tesla China insurance registrations are up about 14 percent from\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "[-0.88435125]\n"
     ]
    }
   ],
   "source": [
    "sample_text = \" \".join(data[datetime.datetime(2024, 6, 25, 0, 0, 0).date()][6])\n",
    "print(sample_text)\n",
    "\n",
    "prediction = model.predict(np.array([sample_text]).astype(object))\n",
    "print(prediction[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d08e03-0e73-4807-84db-ae89a23182d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
