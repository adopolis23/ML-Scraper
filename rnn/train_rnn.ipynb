{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ddffa3-98c9-473e-8b9f-cfc3ec211720",
   "metadata": {},
   "source": [
    "# RNN Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122aa8b8-b84e-4592-ad87-157ef3171ad5",
   "metadata": {},
   "source": [
    "### Setup and Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c781c3ef-0c54-4c2a-b23f-90b396d21f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "#tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "126960a0-bd43-418f-9233-b9c273678f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42 # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92e9b72d-a767-4b1f-8e61-ad03128713ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ed195-cd58-421f-a2a0-e8edaa3da794",
   "metadata": {},
   "source": [
    "### Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "234100a2-e490-4dba-a5fd-04e5f2add187",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "\n",
    "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
    "                                  untar=True, cache_dir='.',\n",
    "                                  cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "# remove unused folders to make it easier to load the data\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c86612c-b298-4d41-936a-17c03f834cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 20000 reviews for train\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# 5000 reviews for validation\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# 25000 reviews for test\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test',\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b23cbee0-6459-4111-a3a6-94b31ac52fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for example, label in train_dataset.take(1):\n",
    "  #print('text: ', example.numpy())\n",
    "  #print('label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a11c23-3702-4243-a862-1ae34836ecb8",
   "metadata": {},
   "source": [
    "### Vocab Encoder for Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ac35c7c-65da-48e0-916e-64d33945a959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
       "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
       "      dtype='<U14')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_ds.map(lambda text, label: text))\n",
    "\n",
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d0a6942-69cc-4f8e-bc1f-61bf0c4e8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example = train_ds.take(1)\n",
    "#encoded_example = encoder(example)[:3].numpy()\n",
    "#encoded_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25a384b-be14-4eaa-9ef0-e6c64ff9e3c8",
   "metadata": {},
   "source": [
    "### Custom Callback Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0e17e7d-8da8-4cab-8fde-e4ed98ea37e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint callback for \"baseline\" performance\n",
    "#finds model weights from iteration with lowest validaion loss\n",
    "class checkpoint(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.min_loss = float(\"inf\")\n",
    "        self.opt_weight = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs['loss']\n",
    "\n",
    "        if val_loss < self.min_loss:\n",
    "            self.min_loss = val_loss\n",
    "            self.opt_weight = self.model.get_weights()\n",
    "\n",
    "            print(\"Validation loss improved to {}, saving weights.\".format(val_loss))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.model.set_weights(self.opt_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d8224b-c7ad-4e83-b24f-8027ac842efb",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dac07bb8-d0b0-40ba-b99c-23a638804c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2468fe3-2d71-4bea-883e-5678d78f5ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "print([layer.supports_masking for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8668e1b2-a3df-41f3-83c9-b418d072d122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 825ms/step\n",
      "[0.00327058]\n"
     ]
    }
   ],
   "source": [
    "sample_text = ('The movie was cool. The animation and the graphics '\n",
    "               'were out of this world. I would recommend this movie.')\n",
    "predictions = model.predict(np.array([sample_text]).astype(object))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65606759-c079-4335-9e2d-9728fb73fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf5faaf-41fe-40f2-945e-b2bf80f48a9b",
   "metadata": {},
   "source": [
    "### Train and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e82b94f8-031e-4185-a8a1-42f8d6fe48e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.7665 - loss: 0.4925Validation loss improved to 0.4564778804779053, saving weights.\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 147ms/step - accuracy: 0.7665 - loss: 0.4925 - val_accuracy: 0.8219 - val_loss: 0.3981\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8258 - loss: 0.3833Validation loss improved to 0.37341824173927307, saving weights.\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 150ms/step - accuracy: 0.8258 - loss: 0.3833 - val_accuracy: 0.8302 - val_loss: 0.3594\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8520 - loss: 0.3475Validation loss improved to 0.3428109884262085, saving weights.\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 152ms/step - accuracy: 0.8520 - loss: 0.3475 - val_accuracy: 0.8365 - val_loss: 0.3429\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8554 - loss: 0.3300Validation loss improved to 0.3260621726512909, saving weights.\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 154ms/step - accuracy: 0.8554 - loss: 0.3300 - val_accuracy: 0.8417 - val_loss: 0.3465\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8639 - loss: 0.3189Validation loss improved to 0.31738147139549255, saving weights.\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 152ms/step - accuracy: 0.8639 - loss: 0.3189 - val_accuracy: 0.8167 - val_loss: 0.3630\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8686 - loss: 0.3125Validation loss improved to 0.3116423189640045, saving weights.\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 151ms/step - accuracy: 0.8686 - loss: 0.3125 - val_accuracy: 0.8750 - val_loss: 0.2821\n",
      "Epoch 7/10\n",
      "\u001b[1m  1/625\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 131ms/step - accuracy: 0.8438 - loss: 0.4004"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Obung\\AppData\\Local\\miniconda3\\envs\\reddit\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8709 - loss: 0.3094Validation loss improved to 0.30771157145500183, saving weights.\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 150ms/step - accuracy: 0.8709 - loss: 0.3094 - val_accuracy: 0.8479 - val_loss: 0.3334\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8727 - loss: 0.3063Validation loss improved to 0.304545521736145, saving weights.\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 150ms/step - accuracy: 0.8727 - loss: 0.3063 - val_accuracy: 0.8333 - val_loss: 0.3393\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8730 - loss: 0.3041Validation loss improved to 0.30203619599342346, saving weights.\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 151ms/step - accuracy: 0.8730 - loss: 0.3041 - val_accuracy: 0.8417 - val_loss: 0.3345\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8738 - loss: 0.3018Validation loss improved to 0.2994726002216339, saving weights.\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 152ms/step - accuracy: 0.8738 - loss: 0.3018 - val_accuracy: 0.8427 - val_loss: 0.3463\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=10,\n",
    "                    validation_data=val_ds,\n",
    "                    validation_steps=30,\n",
    "                    callbacks=[checkpoint()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd1617a3-4c1d-450c-b494-9cb31fa24023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - accuracy: 0.8568 - loss: 0.3164\n",
      "Test Loss: 0.3225228786468506\n",
      "Test Accuracy: 0.8510400056838989\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b814a103-29fc-451c-9109-d751bcb5d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('Models/model1.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b26d9dc-a44e-4c79-b03c-c8e4685db34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('Models/model1.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba892875-7a3d-4732-8760-104bf954cba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
